{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77249ba",
   "metadata": {},
   "source": [
    "基于Bert-Crf中文命名实体识别算法鲁棒性评测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ebb5b",
   "metadata": {},
   "source": [
    "赛题链接：https://www.datafountain.cn/competitions/510\n",
    "数据说明数据集采用OntoNotes 4.0 数据集 (Weischedel et al., 2011)，数据集未分词，标签为(B,M,E)-(PER,LOC,ORG,GPE)的组合。\n",
    "\n",
    "评测所用的验证集和测试集中部分句子由鲁棒性验证工具 TextFlint 产生，共有11种变形方式   \n",
    "在句中随机加入标点符号，两阶段均使用；    \n",
    "对句中任意单词做同义词替换，两阶段均使用；    \n",
    "对句中任意单词做反义词替换，两阶段均使用；    \n",
    "将句中可缩写的词语替换为缩写词，两阶段均使用；    \n",
    "将句中数字随机变为其他数值，两阶段均使用；    \n",
    "在句中随机插入语义无关的句子，两阶段均使用；    \n",
    "将句中部分汉字换为同音字，仅第二阶段使用；    \n",
    "在句中合适位置插入副词，仅第二阶段使用；    \n",
    "将句中实体换为同类型的、更长的实体，仅第二阶段使用；    \n",
    "将句中实体换为交叉类型词（如将渥太华ORG换为华盛顿ORG&PER），仅第二阶段使用；    \n",
    "将句中实体换为训练集中不曾出现过的实体，仅第二阶段使用。\n",
    "\n",
    "评测标准：我们通过将输出结果与人工标注的集合进行比较来分别计算每一种元素准确率(Precision)，召回率(Recall)和F-1分值(F-1 score)，并采用Micro-F1作为最终排名指标。Baseline概述：使用Bert+crf算法，以序列标注的形式来进行命名实体识别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3888f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=5.3.0 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from torchvision) (1.20.1)\n",
      "Requirement already satisfied: fastNLP in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from fastNLP) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from fastNLP) (4.59.0)\n",
      "Requirement already satisfied: prettytable>=0.7.2 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from fastNLP) (2.1.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from fastNLP) (1.9.0)\n",
      "Requirement already satisfied: spacy in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from fastNLP) (3.1.1)\n",
      "Requirement already satisfied: numpy>=1.14.2 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from fastNLP) (1.20.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from fastNLP) (2021.4.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from prettytable>=0.7.2->fastNLP) (0.2.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from torch>=1.0.0->fastNLP) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from requests->fastNLP) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from requests->fastNLP) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from requests->fastNLP) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from requests->fastNLP) (2020.12.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from spacy->fastNLP) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from spacy->fastNLP) (8.0.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from spacy->fastNLP) (0.7.4)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from spacy->fastNLP) (0.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from spacy->fastNLP) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from spacy->fastNLP) (3.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from spacy->fastNLP) (0.6.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from spacy->fastNLP) (0.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from spacy->fastNLP) (2.11.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from spacy->fastNLP) (1.8.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from spacy->fastNLP) (52.0.0.post20210125)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from spacy->fastNLP) (1.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from spacy->fastNLP) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from spacy->fastNLP) (20.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from spacy->fastNLP) (2.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy->fastNLP) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy->fastNLP) (5.1.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy->fastNLP) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from jinja2->spacy->fastNLP) (1.1.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (4.9.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: six in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\stellaliu\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio\n",
    "!pip install fastNLP\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e6a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "import fastNLP\n",
    "from fastNLP import SpanFPreRecMetric, Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba83b1af",
   "metadata": {},
   "source": [
    "数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d05da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nerdataset(Dataset):\n",
    "    def _init_(self, path):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "        self.device = 'cude:3'\n",
    "        self.label2idx = { 'O': 0, 'B-LOC': 1, 'M-LOC': 2, 'E-LOC': 3,\n",
    "                          'S-LOC': 4, 'B-PER': 5, 'M-PER': 6, 'E-PER': 7,\n",
    "                          'S-PER': 8, 'B-GPE': 9, 'M-GPE': 10, 'E-GPE': 11,\n",
    "                          'S-GPE': 12, 'B-ORG': 13, 'M-ORG': 14, 'E-ORG': 15,\n",
    "                          'S-ORG': 16 }\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese').to(self.device)\n",
    "        self.examples = self.processor(path)\n",
    "    def __len__(self):\n",
    "        return len(self.examples[0])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.examples[0][index], self.examples[1][index]\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        data.sort(key=lambda x: - len(x[1]))\n",
    "        data = list(zip(*data))\n",
    "        return tuple(data)\n",
    "    \n",
    "    def processor(self, path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = lines[1:]\n",
    "            sentence = ''\n",
    "            label = []\n",
    "            s = []\n",
    "            l = []\n",
    "            for line in tqdm.tqdm(lines):\n",
    "                if len(line) > 2:\n",
    "                    line = line.strip().split(',')\n",
    "                    sentence += line[0]\n",
    "                    label += [line[1]]\n",
    "                else:\n",
    "                    tokens = ['[CLS]'] + [self.tokenizer.tokenize(i)[0] for\n",
    "                                          i in sentence] + ['SEP']\n",
    "                    if len(tokens) > 512:\n",
    "                        sentence = []\n",
    "                        label = []\n",
    "                        continue\n",
    "                    ids = torch.tensor(\n",
    "                        [self.tokenizer.convert_tokens_to_ids(tokens)])\n",
    "                    assert ids.shape[1] - 2 == len(sentence) \\\n",
    "                           and len(sentence) == len(label)\n",
    "                    s.append(ids)\n",
    "                    l.append([self.label2idx[i] for i in label])\n",
    "                    sentence = ''\n",
    "                    label = []\n",
    "            return [s, l]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db06300",
   "metadata": {},
   "source": [
    "bert crf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6838426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NER, self).__init__()\n",
    "        self.device = 'cuda:3'\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese').to(\n",
    "            self.device)\n",
    "        self.encode_lstm = torch.nn.LSTM(input_size=768, hidden_size=768,\n",
    "                                  batch_first=True, bidirectional=True).to(\n",
    "                                  self.device)\n",
    "        self.crf = fastNLP.modules.ConditionalRandomField(17).to(self.device)\n",
    "        self.decode = torch.nn.Linear(768 * 2, 17)\n",
    "        \n",
    "    def forward(self, Data):\n",
    "        data, label = Data\n",
    "        l = [len(i) for i in label]\n",
    "        m = 0\n",
    "        for i in l:\n",
    "            m = max(m, i)\n",
    "        label = torch.tensor([i + [-1] * (m - len(i)) for i in label])\n",
    "        data = torch.stack([torch.cat(\n",
    "            [self.bert(i.to('cuda:3'))[0][0][1:-1]] + [\n",
    "                torch.tensor([0] * 768).unsqueeze(dim=0).to('cuda:3')] * (\n",
    "                        m - l[j]), dim=0) for j, i in enumerate(data)])\n",
    "        data = torch.nn.utils.rnn.pack_padded_sequence(data, l,\n",
    "                                                       batch_first=True)\n",
    "        data, _ = self.encode_lstm(data)\n",
    "        data, _ = torch.nn.utils.rnn.pad_packed_sequence(data)\n",
    "        data = data.transpose(0, 1)\n",
    "\n",
    "        data = self.decode(data)\n",
    "        if self.training:\n",
    "            return self.crf.forward(data, label, torch.BoolTensor(\n",
    "                [[1] * i + [0] * (m - i) for i in l]).to(self.device))\n",
    "        return self.crf.viterbi_decode(data, torch.BoolTensor(\n",
    "            [[1] * i + [0] * (m - i) for i in l]).to(self.device))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f66c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, epoch, optimizer):\n",
    "    net.to('cuda:3')\n",
    "    n = 0\n",
    "    best = 0\n",
    "    i = 0\n",
    "    while n < 10:\n",
    "        i += 1\n",
    "        net.train()\n",
    "        tot_loss = 0\n",
    "        with tqdm.tqdm(total=len(train_iter), desc='training') as pbar:\n",
    "            for Data in train_iter:\n",
    "                data, label = Data\n",
    "                optimizer.zero_grad()\n",
    "                all = 0\n",
    "                for j in label:\n",
    "                    all += len(j)\n",
    "                loss = net(Data).sum() / all\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tot_loss += loss.item()\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"loss\": loss.item(), 'lr':\n",
    "                    optimizer.state_dict()['param_groups'][0]['lr']})\n",
    "        print(f\"epoch {i}: loss: {tot_loss / len(train_iter)}\")\n",
    "        # evaluate_accuracy(train_iter, net, i)\n",
    "        score = evaluate_accuracy(test_iter, net, i)\n",
    "        score = evaluate_accuracy(test_iter, net, i)\n",
    "        n += 1\n",
    "        if score['f'] > best:\n",
    "            torch.save(net, 'best')\n",
    "            best = score['f']\n",
    "            n = 0\n",
    "        print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbfdeef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, epoch):\n",
    "    net.eval()\n",
    "    vocab = Vocabulary(unknown=None, padding=None)\n",
    "    vocab.add_word_lst(['O', 'B-LOC', 'M-LOC', 'E-LOC',\n",
    "                        'S-LOC', 'B-PER', 'M-PER', 'E-PER',\n",
    "                        'S-PER', 'B-GPE', 'M-GPE', 'E-GPE',\n",
    "                        'S-GPE', 'B-ORG', 'M-ORG', 'E-ORG',\n",
    "                        'S-ORG'])\n",
    "    metrics = SpanFPreRecMetric(tag_vocab=vocab,\n",
    "                                encoding_type='bmeso', f_type='macro')\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm.tqdm(data_iter):\n",
    "            pred = net(data)\n",
    "            label = data[1]\n",
    "            l = [len(i) for i in label]\n",
    "            m = 0\n",
    "            for i in l:\n",
    "                m = max(m, i)\n",
    "            label = torch.tensor([i + [-1] * (m - len(i)) for i in label])\n",
    "            metrics.evaluate(pred, label,\n",
    "                             torch.tensor(l))\n",
    "    return metrics.get_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------------------- Process train data -------------------------')\n",
    "train_iter = torch.utils.data.DataLoader(dataset=Nerdataset('ner/train.csv'), batch_size=20, shuffle=True,\n",
    "    collate_fn=Nerdataset.collate_fn)\n",
    "print('--------------------- Process test data -------------------------')\n",
    "test_iter = torch.utils.data.DataLoader(dataset=Nerdataset('ner/A.csv'), batch_size=20, shuffle=False,\n",
    "    collate_fn=Nerdataset.collate_fn)\n",
    "\n",
    "print('--------------------- Start training -------------------------')\n",
    "ner = NER()\n",
    "bert_params = list(map(id, ner.bert.parameters()))\n",
    "logits_params = filter(lambda p: id(p) not in bert_params, ner.parameters())\n",
    "params = [\n",
    "    {\"params\": logits_params, \"lr\": 1e-3},\n",
    "    {\"params\": ner.bert.parameters(), \"lr\": 1e-5}]\n",
    "\n",
    "optimizer = torch.optim.Adam(params)\n",
    "train(ner, train_iter, test_iter, 30, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
